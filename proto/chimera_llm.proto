syntax = "proto3";

service LLM {
    rpc Inspect(InspectRequest) returns (InspectResponse) {}
    rpc LoadModel(LoadModelRequest) returns (LoadModelResponse) {}
    rpc Completion(CompletionRequest) returns (CompletionPrediction) {}
    rpc Chat(ChatRequest) returns (ChatPrediction) {}
}

enum ModelType{
    MODEL_TYPE_UNKNOWN = 0;
    GENERAL = 1;
    CHAT = 2;
    TEXT = 3;
    CODE = 4;
}
enum Status{
    STATUS_UNKNOWN = 0;
    INITLIZING = 1;
    READY = 2;
    ERROR = 3;
}

message InspectRequest {}
message AvaliableModel {
    repeated ModelType model_type = 1;
    string model_id = 2;
    string model_name = 3;
    string size = 4;
    string model_description = 5;
}
message InspectResponse {
    repeated AvaliableModel avaliable_models = 1;
    Status current_status = 2;
}

message LoadModelRequest {
    string model_id = 1;
}
message LoadModelResponse {
    string model_id = 1;
    Status current_status = 2;
}

enum Role {
    ROLE_UNKNOWN = 0;
    SYSTEM = 1;
    USER = 2;
    ASSISTANT = 3;
}

message CommonArgs {
    float temperature = 1;
    float top_p = 2;
    uint32 max_gen_len = 3;
    uint32 repetition_penalty = 5;
}

message CompletionRequest{
    string request_id = 1;
    string prompt = 2;
    CommonArgs common_args = 3;
}

message CompletionPrediction{
    string request_id = 1;
    string response_id = 2;
    string generation = 3;
}

message ChatMessage{
    Role role = 1;
    string content = 2;
}

message ChatRequest{
    string request_id = 1;
    repeated ChatMessage messages = 2;
    CommonArgs common_args = 3;
}

message ChatPrediction{
    string request_id = 1;
    string response_id = 2;
    ChatMessage message = 3;
}
